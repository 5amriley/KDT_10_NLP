{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AGNEWS RNN 모델 생성, 학습\n",
    "- [1] 데이터 전처리\n",
    "    - (한글 제외 모두 제거, 줄임말 한 가지로 처리) ~ skip\n",
    "    - 토큰화\n",
    "    - 토큰 데이터 정수 인코딩, 데이터 길이 표준화(패딩)\n",
    "- [2] 학습 준비\n",
    "    - Dataset, DataLoader 준비\n",
    "    - 모델 클래스\n",
    "    - 학습, 테스트 함수\n",
    "    - 학습 관련 변수 => DEVICE, OPTIMIZER, MODEL 인스턴스, EPOCHS, BATCH_SIZE, LOSS_FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*미완성 코드*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchdata\n",
    "import torch\n",
    "from torchtext.datasets import AG_NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")\n",
      "(3, 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.')\n",
      "(3, \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.\")\n",
      "(3, 'Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\\\flows from the main pipeline in southern Iraq after\\\\intelligence showed a rebel militia could strike\\\\infrastructure, an oil official said on Saturday.')\n",
      "(3, 'Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.')\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for a in AG_NEWS(split='train'):\n",
    "    cnt += 1\n",
    "    print(a)    # <class 'tuple'> ~ 길이는 2\n",
    "    if cnt == 5: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "### 토크나이저 생성\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "### 뉴스 학습 데이터 추출\n",
    "train_iter = AG_NEWS(split='train')\n",
    "\n",
    "### 뉴스 테스트 데이터 추출\n",
    "test_iter = AG_NEWS(split='test')\n",
    "\n",
    "# DataPipe 타입 >>> generator 타입 형변환\n",
    "# train_iter = iter(AG_NEWS(split='train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 한국어라면\n",
    "# from konlpy.tag import Okt\n",
    "# tokenizer = Okt()\n",
    "# 그리고, yield_tokens에서 tokenizer.morphs(text) 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 토큰 제너레이터 함수 : 데이터 추출하여 토큰화\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        # 라벨, 텍스트 -> 텍스트 토큰화\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 단어사전 생성\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter),\n",
    "                                  specials=['<PAD>', '<UNK>'])\n",
    "\n",
    "### <UNK> 인덱스 0으로 설정\n",
    "vocab.set_default_index(vocab['<UNK>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[283, 916, 5, 364, 161, 0, 1]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['i', 'want', 'to', 'go', 'home', '<PAD>', '<UNK>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[282, 915, 4, 363, 160]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 텍스트 >>> 정수 인코딩\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "text_pipeline('I want to go home')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 레이블 >>> 정수 인코딩\n",
    "### 1 ~ 4 를 0 ~ 3 으로 바꾸려고\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 가공\n",
    "- 토큰 데이터 정수 인코딩\n",
    "- 데이터 길이 표준화 => 데이터의 길이 맞추기 (1개 문장 구성하는 단어 수 통일)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.vocab.vocab.Vocab'>\n"
     ]
    }
   ],
   "source": [
    "print(type(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95812\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = vocab.get_stoi()  # dict (인코딩)\n",
    "id_to_token = vocab.get_itos()  # list (디코딩)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAD>', '<UNK>', '.', 'the', ',', 'to', 'a', 'of', 'in', 'and']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_token[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID = token_to_id[\"<PAD>\"]   # 0\n",
    "UNK_ID = token_to_id[\"<UNK>\"]   # 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 패딩 처리 함수\n",
    "def pad_sequence(sentences, max_length, pad, start='R'):\n",
    "    result = []\n",
    "    for sen in sentences:\n",
    "        sen = sen[:max_length] if start == 'R' else sen[-1 * max_length:]\n",
    "        pad_length = max_length - len(sen)  # sen이 원래 max_length 보다 짧았다면 1이상의 값 저장\n",
    "        padd_sen = sen + [pad]*pad_length if start == 'R' else [pad]*pad_length + sen\n",
    "        result.append(padd_sen)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 함수 사용하여 일정한 길이의 정수 인코딩 데이터 생성\n",
    "MAX_LENGTH = 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 배치(batch)와 반복자 생성\n",
    "- torch.utils.data.DataLoader : getitem(), len() 구현한 맵 형태(map-style)\n",
    "- collate_fn() : DataLoader로부터 생성된 샘플 배치 함수\n",
    "    - 입력 : DataLoader에 배치 크기(batch size)가 있는 배치(batch) 데이터\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "### 배치크기만큼 데이터셋 반환 함수\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]    # offset : 파일 포인터\n",
    "\n",
    "    # 뉴스기사, 라벨을 1개씩 추출해서 저장\n",
    "    for (_label, _text) in batch:\n",
    "        # 라벨 인코딩 후 저장\n",
    "        label_list.append(label_pipeline(_label))\n",
    "\n",
    "        # 텍스트 인코딩 후 저장\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "\n",
    "        # 텍스트 offset 즉, 텍스트 크기/길이 저장\n",
    "        offsets.append(processed_text.size(0))\n",
    "    \n",
    "    # 텐서화 진행\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_list : tensor([2, 2, 2, 2, 2])\n",
      "text_list : tensor([  431,   425,     1,  1605, 14838,   113,    66,     2,   848,    13,\n",
      "           27,    14,    27,    15, 50725,     3,   431,   374,    16,     9,\n",
      "        67507,     6, 52258,     3,    42,  4009,   783,   325,     1, 15874,\n",
      "         1072,   854,  1310,  4250,    13,    27,    14,    27,    15,   929,\n",
      "          797,   320, 15874,    98,     3, 27657,    28,     5,  4459,    11,\n",
      "          564, 52790,     8, 80617,  2125,     7,     2,   525,   241,     3,\n",
      "           28,  3890, 82814,  6574,    10,   206,   359,     6,     2,   126,\n",
      "            1,    58,     8,   347,  4582,   151,    16,   738,    13,    27,\n",
      "           14,    27,    15,  2384,   452,    92,  2059, 27360,     2,   347,\n",
      "            8,     2,   738,    11,   271,    42,   240, 51953,    38,     2,\n",
      "          294,   126,   112,    85,   220,     2,  7856,     6, 40066, 15380,\n",
      "            1,    70,  7376,    58,  1810,    29,   905,   537,  2846,    13,\n",
      "           27,    14,    27,    15,   838,    39,  4978,    58, 68871,    29,\n",
      "            2,   905,  2846,     7,   537,    70, 58874,   703,     5,   912,\n",
      "         2520,    93, 89171,     3,    30,    58,   293,    26,    10,   114,\n",
      "            1,    58,    92,  4379,     4,  3581,   145,     3,  7577,    23,\n",
      "        12282,     4,    36,   347,    13,   105,    14,   105,    15, 90056,\n",
      "           50,    58,    92,     3, 11312,  1732,     8, 13750,  9735,     3,\n",
      "         3593,     5,    23,   365, 12282,  3470,    94,   299,   167,     2,\n",
      "           36,   399,   545,     1])\n",
      "text_list.shape : torch.Size([194])\n",
      "offsets : tensor([  0,  29,  71, 111, 151])\n"
     ]
    }
   ],
   "source": [
    "### collate_batch 함수 이해하기 위한 테스트 코드\n",
    "\"\"\"cnt = 0\n",
    "test_list = []\n",
    "\n",
    "# AG_NEWS 데이터셋 5개 데이터만 추출, 저장\n",
    "for x in AG_NEWS(split='train'):\n",
    "    if cnt == 5: break\n",
    "    # print(x)\n",
    "    test_list.append(x)\n",
    "    cnt += 1\n",
    "\n",
    "label_list, text_list, offsets = collate_batch(test_list)\n",
    "\n",
    "print(f'label_list : {label_list}')\n",
    "print(f'text_list : {text_list}')\n",
    "print(f'text_list.shape : {text_list.shape}')\n",
    "print(f'offsets : {offsets}')\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 데이터로더 생성\n",
    "train_iter = AG_NEWS(split='train')\n",
    "trainDL = DataLoader(train_iter,\n",
    "                     batch_size=16,\n",
    "                     shuffle=True,\n",
    "                     collate_fn=collate_batch)  # collate_fn 역할? 최종 데이터 형식?\n",
    "testDL = DataLoader(test_iter,\n",
    "                    batch_size=16,\n",
    "                    shuffle=True,\n",
    "                    collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_class : 4     vocab_size : 95811\n"
     ]
    }
   ],
   "source": [
    "### 분류 클래스 수와 단어사전 개수\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f'num_class : {num_class}     vocab_size : {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            hidden_dim,\n",
    "            embedding_dim,\n",
    "            n_layers,\n",
    "            dropout=0.5,\n",
    "            bidirectional=True,\n",
    "            model_type='lstm'\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        if model_type == 'rnn':\n",
    "            self.model = nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        elif model_type == 'lstm':\n",
    "            self.model = nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim * 2, 1) if bidirectional else nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEVICE, CLASSIFIER, CRITERION, OPTIMIZER\n",
    "from torch import optim\n",
    "\n",
    "vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Text_017_220_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
