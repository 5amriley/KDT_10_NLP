{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65ed6b5db40d3c12",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "[ Soynlp ] 학습 기반 토크나이저\n",
    "- 품사 태깅, 단어 토큰화 등을 지원하는 단어 토크나이저\n",
    "- 비비도 학습으로 단어 토큰화 => 데이터에 자주 등장하는 단어들을 단어로 분석\n",
    "- 내부적으로 단어 점수 표로 동작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a38adb04e5a416c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T00:07:43.564624400Z",
     "start_time": "2024-04-01T00:07:39.992870500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting soynlp\n",
      "  Downloading soynlp-0.0.493-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: numpy>=1.12.1 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from soynlp) (1.24.3)\n",
      "Requirement already satisfied: psutil>=5.0.1 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from soynlp) (5.9.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from soynlp) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from soynlp) (1.3.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from scikit-learn>=0.20.0->soynlp) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from scikit-learn>=0.20.0->soynlp) (2.2.0)\n",
      "Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n",
      "   ---------------------------------------- 0.0/416.8 kB ? eta -:--:--\n",
      "   --- ----------------------------------- 41.0/416.8 kB 653.6 kB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 112.6/416.8 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 194.6/416.8 kB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 256.0/416.8 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 358.4/416.8 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 416.8/416.8 kB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: soynlp\n",
      "Successfully installed soynlp-0.0.493\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install soynlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2fac42b8b1864e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "[기존 형태소 분석기] : 신조어나 형태소 분석기에 등록되지 않은 단어는 제대로 구분하지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f55e4164d5940069",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T00:21:40.668042600Z",
     "start_time": "2024-04-01T00:21:40.630864500Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['에이', '비식스', '이대', '휘', '1월', '최애', '돌', '기부', '요정', '입니다']\n",
      "['에이', '비식스', '이대', '휘', '1월', '최애', '돌', '기부', '요정', '이다']\n",
      "['에이', '비식스', '이대', '휘', '1월', '최애', '돌', '기부', '요정', '입니다']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "tokenizer = Okt()\n",
    "\n",
    "# 형태소 분석 시 매개변수 stem=True, norm=True 설정\n",
    "print(tokenizer.morphs('에이비식스 이대휘 1월 최애돌 기부 요정입니다'))\n",
    "print(tokenizer.morphs('에이비식스 이대휘 1월 최애돌 기부 요정입니다', stem=True))  # 어간\n",
    "print(tokenizer.morphs('에이비식스 이대휘 1월 최애돌 기부 요정입니다', norm=True))  # 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d78db0c14dfcc24",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "[Soynlp] 사용 => 말뭉치 데이터셋으로 학습 진행 후 사용 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b6918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 학습 데이터 처리\n",
    "from soynlp import DoublespaceLineCorpus    # 한 개로 통합된 문서 데이터 분리\n",
    "from soynlp.word import WordExtractor       # 단어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10521894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "news_url = \"https://raw.githubusercontent.com/lovit/soynlp/master/tutorials/2016-10-20.txt\"\n",
    "filename = '../datasets/news_data.txt'\n",
    "\n",
    "ret = urlretrieve(url=news_url, filename=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c90d272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 문서 : 30091개\n"
     ]
    }
   ],
   "source": [
    "### 훈련 데이터 문서 분리\n",
    "corpus = DoublespaceLineCorpus(filename)\n",
    "print(f'훈련 데이터 문서 : {len(corpus)}개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f51efe86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training was done. used memory 1.110 Gb\n",
      "all cohesion probabilities was computed. # words = 223348\n",
      "all branching entropies was computed # words = 361598\n",
      "all accessor variety was computed # words = 361598\n"
     ]
    }
   ],
   "source": [
    "### soynlp 학습 진행\n",
    "word_extractor = WordExtractor()\n",
    "\n",
    "# 학습 진행하며 단어별 점수 생성\n",
    "word_extractor.train(corpus)\n",
    "\n",
    "# 단어별 점수표 추출\n",
    "word_score_table = word_extractor.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4134ad4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어별 점수표 확인\n",
    "for idx, key in enumerate(word_score_table.keys()):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc2b1b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 응집 확률(cohesion probability) : 내부 문자열(substring)이 얼마나 응집하여 자주 등장하는지를 판단하는 척도\n",
    "# - 원리 : 문자열을 문자 단위로 분리, 왼쪽부터 순서대로 문자를 추가\n",
    "#          각 문자열이 주어졌을 때 그 다음 문자가 나올 확률을 계산 / 누적곱 한 값\n",
    "# - 값이 높을수록 : 전체 코퍼스에서 이 문자열 시퀀스는 하나의 단어로 등장할 가능성 높음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8866d00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table['바'].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dc32600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06393648140409527"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table['바다'].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0db4051c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07716779358040307"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table['바다를'].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "193b4268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11518621707955429"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_score_table['바다에'].cohesion_forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5057f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### soynlp의 L tokenizer\n",
    "# 띄어쓰기 단위로 나눈 어절 토큰 : L 토큰 + R 토큰\n",
    "# 예 : '공원에' -> '공원 + 에', '공부하는' -> '공부 + 하는'\n",
    "# 분리 기준 : 점수가 가장 높은 L 토큰을 찾아내는 원리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "71e0febf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('국제사회', '와'), ('우리', '의'), ('노력', '들로'), ('범죄', '를'), ('척결', '하자')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from soynlp.tokenizer import LTokenizer\n",
    "\n",
    "scores = {word:score.cohesion_forward for word, score in word_score_table.items()}\n",
    "\n",
    "L_tokenizer = LTokenizer(scores=scores)\n",
    "L_tokenizer.tokenize('국제사회와 우리의 노력들로 범죄를 척결하자', flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9c8bbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['국제사회', '와', '우리', '의', '노력', '들로', '범죄', '를', '척결', '하자']\n",
      "['국제사회', '와우', '리의', '노력', '들로', '범죄', '를', '척결', '하자']\n"
     ]
    }
   ],
   "source": [
    "### 최대 점수 토크나이저\n",
    "# - 띄어쓰기가 되지 않는 문장에서 점수가 높은 글자 시퀀스를 순차적으로 찾아내는 토크나이저\n",
    "# - 띄어쓰기가 되어 있지 않은 문장을 넣어서 점수를 통해 토큰화 된 결과\n",
    "from soynlp.tokenizer import MaxScoreTokenizer\n",
    "\n",
    "maxscore_tokenizer = MaxScoreTokenizer(scores=scores)\n",
    "print(maxscore_tokenizer.tokenize('국제사회와우리의노력들로범죄를척결하자'\n",
    "                            # , flatten=False\n",
    "                            ))\n",
    "print(maxscore_tokenizer.tokenize('국제사회 와우 리의노력들로범죄를척결하자'\n",
    "                            # , flatten=False\n",
    "                            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db57f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### soynlp를 이용한 반복되는 문자 정제\n",
    "# - ㅋㅋ, ㅎㅎ 등의 이모티콘의 경우 불필요하게 연속되는 경우가 많음\n",
    "# - ㅋㅋ, ㅋㅋㅋ, ㅋㅋㅋㅋ 와 같은 경우를 모두 서로 다른 단어로 처리하는 것은 불필요\n",
    "# >>> 반복되는 것은 하나로 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8e61cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아ㅋ영화존잼ㅠ\n",
      "아ㅋㅋ영화존잼ㅠㅠ\n",
      "아ㅋㅋ영화존잼ㅠㅠ\n"
     ]
    }
   ],
   "source": [
    "from soynlp.normalizer import *\n",
    "\n",
    "print(emoticon_normalize('앜ㅋㅋㅋ이영화존잼ㅠㅠㅠㅠㅠ', num_repeats=1))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋ그영화존잼ㅠㅠㅠㅠㅠ', num_repeats=2))\n",
    "print(emoticon_normalize('앜ㅋㅋㅋㅋㅋㅋㅋㅋ저영화존잼ㅠㅠㅠㅠㅠㅠ', num_repeats=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74fc1948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "와하하하핫\n",
      "와하하핫\n"
     ]
    }
   ],
   "source": [
    "print(repeat_normalize('와하하하핫', num_repeats=2))\n",
    "print(repeat_normalize('와하하하하하핫', num_repeats=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7216f3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting customized_konlpy\n",
      "  Downloading customized_konlpy-0.0.64-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: Jpype1>=0.6.1 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from customized_konlpy) (1.4.1)\n",
      "Requirement already satisfied: konlpy>=0.4.4 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from customized_konlpy) (0.6.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from Jpype1>=0.6.1->customized_konlpy) (23.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from konlpy>=0.4.4->customized_konlpy) (5.1.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\programdata\\anaconda3\\envs\\torch_nlp38\\lib\\site-packages (from konlpy>=0.4.4->customized_konlpy) (1.24.3)\n",
      "Downloading customized_konlpy-0.0.64-py3-none-any.whl (881 kB)\n",
      "   ---------------------------------------- 0.0/881.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 10.2/881.5 kB ? eta -:--:--\n",
      "   - ------------------------------------- 41.0/881.5 kB 495.5 kB/s eta 0:00:02\n",
      "   ----- -------------------------------- 122.9/881.5 kB 901.1 kB/s eta 0:00:01\n",
      "   --------- ------------------------------ 204.8/881.5 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 430.1/881.5 kB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 604.2/881.5 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 881.5/881.5 kB 2.8 MB/s eta 0:00:00\n",
      "Installing collected packages: customized_konlpy\n",
      "Successfully installed customized_konlpy-0.0.64\n"
     ]
    }
   ],
   "source": [
    "# !pip install customized_konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2d9f875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\Torch_NLP38\\lib\\site-packages\\konlpy\\tag\\_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
      "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['은', '경이', '는', '사무실', '로', '갔습니다']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ckonlpy.tag import Twitter\n",
    "\n",
    "twitter = Twitter()\n",
    "twitter.morphs('은경이는 사무실로 갔습니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00628dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('은', 'Noun'),\n",
       " ('경이', 'Noun'),\n",
       " ('는', 'Josa'),\n",
       " ('사무실', 'Noun'),\n",
       " ('로', 'Josa'),\n",
       " ('갔습니다', 'Verb')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.pos('은경이는 사무실로 갔습니다')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02046193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소분석기에 사전 추가\n",
    "twitter.add_dictionary('은경이', 'Noun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f01e2da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['은경이', '는', '사무실', '로', '갔습니다']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.morphs('은경이는 사무실로 갔습니다')    # 추가사항 반영"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b41734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
