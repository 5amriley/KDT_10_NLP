{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torchtext 라이브러리로 텍스트 분류\n",
    "- (1)단계 - 데이터 전처리 : 숫자형식으로 변환하는 것 까지\n",
    "- (2)단계 - 모델 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1-1) 데이터 준비 => 내장 데이터셋 활용\n",
    "- AG_NEWS 데이터셋 반복자 : 레이블(label) + 문장의 튜플(tuple) 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchdata in c:\\users\\kdp\\appdata\\roaming\\python\\python38\\site-packages (0.7.1)\n",
      "Requirement already satisfied: urllib3>=1.25 in c:\\programdata\\anaconda3\\envs\\text_017_220_38\\lib\\site-packages (from torchdata) (2.1.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\envs\\text_017_220_38\\lib\\site-packages (from torchdata) (2.31.0)\n",
      "Requirement already satisfied: torch>=2 in c:\\programdata\\anaconda3\\envs\\text_017_220_38\\lib\\site-packages (from torchdata) (2.2.2)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\envs\\text_017_220_38\\lib\\site-packages (from torch>=2->torchdata) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\programdata\\anaconda3\\envs\\text_017_220_38\\lib\\site-packages (from torch>=2->torchdata) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\programdata\\anaconda3\\envs\\text_017_220_38\\lib\\site-packages (from torch>=2->torchdata) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\envs\\text_017_220_38\\lib\\site-packages (from torch>=2->torchdata) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\envs\\text_017_220_38\\lib\\site-packages (from torch>=2->torchdata) (3.1.3)\n",
      "Collecting fsspec (from torch>=2->torchdata)\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\envs\\text_017_220_38\\lib\\site-packages (from requests->torchdata) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\envs\\text_017_220_38\\lib\\site-packages (from requests->torchdata) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\envs\\text_017_220_38\\lib\\site-packages (from requests->torchdata) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\envs\\text_017_220_38\\lib\\site-packages (from jinja2->torch>=2->torchdata) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\programdata\\anaconda3\\envs\\text_017_220_38\\lib\\site-packages (from sympy->torch>=2->torchdata) (1.3.0)\n",
      "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "   ---------------------------------------- 0.0/172.0 kB ? eta -:--:--\n",
      "   --------- ----------------------------- 41.0/172.0 kB 991.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 172.0/172.0 kB 2.6 MB/s eta 0:00:00\n",
      "Installing collected packages: fsspec\n",
      "Successfully installed fsspec-2024.3.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "# DataPipe 타입 >>> iterator 타입 형변환\n",
    "train_iter = iter(AG_NEWS(split='train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")\n",
      "(3, 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.')\n",
      "(3, \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.\")\n",
      "(3, 'Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\\\flows from the main pipeline in southern Iraq after\\\\intelligence showed a rebel militia could strike\\\\infrastructure, an oil official said on Saturday.')\n",
      "(3, 'Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.')\n",
      "(3, 'Stocks End Up, But Near Year Lows (Reuters) Reuters - Stocks ended slightly higher on Friday\\\\but stayed near lows for the year as oil prices surged past  #36;46\\\\a barrel, offsetting a positive outlook from computer maker\\\\Dell Inc. (DELL.O)')\n",
      "(3, \"Money Funds Fell in Latest Week (AP) AP - Assets of the nation's retail money market mutual funds fell by  #36;1.17 billion in the latest week to  #36;849.98 trillion, the Investment Company Institute said Thursday.\")\n",
      "(3, 'Fed minutes show dissent over inflation (USATODAY.com) USATODAY.com - Retail sales bounced back a bit in July, and new claims for jobless benefits fell last week, the government said Thursday, indicating the economy is improving from a midsummer slump.')\n",
      "(3, 'Safety Net (Forbes.com) Forbes.com - After earning a PH.D. in Sociology, Danny Bazil Riley started to work as the general manager at a commercial real estate firm at an annual base salary of  #36;70,000. Soon after, a financial planner stopped by his desk to drop off brochures about insurance benefits available through his employer. But, at 32, \"buying insurance was the furthest thing from my mind,\" says Riley.')\n",
      "(3, \"Wall St. Bears Claw Back Into the Black  NEW YORK (Reuters) - Short-sellers, Wall Street's dwindling  band of ultra-cynics, are seeing green again.\")\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for a in AG_NEWS(split='train'):\n",
    "    cnt += 1\n",
    "    print(a)\n",
    "    if cnt == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 확인 => (label, text), label 1~4\n",
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) 데이터 처리 파이프라인 준비\n",
    "- 어휘집(vocab), 단어 벡터(word vector), 토크나이저(tokenizer)\n",
    "- 가공되지 않은 텍스트 문자열에 대한 데이터 처리 빌딩 블록\n",
    "- 일반적인 NLP 데이터 처리\n",
    "    - 첫번째 단계 : 가공되지 않은 학습 데이터셋으로 어휘집 생성\n",
    "        => 토큰 목록 또는 반ㅂ족자 받는 내장 팩토리 함수(factory function) : build_vocab_from_iterator\n",
    "    - 사용자는 어휘집에 추가할 특수 기호(special symbol) 전달 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "### 토크나이저 생성\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "### 뉴스 학습 데이터 추출\n",
    "train_iter = AG_NEWS(split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 토큰 제너레이터 함수 : 데이터 추출하여 토큰화\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        # 라벨, 텍스트 -> 텍스트 토큰화\n",
    "        yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 단어사전 생성 (조각난 단어를 숫자로 바꾸는 역할)\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter),\n",
    "                                  specials=['<unk>'])\n",
    "\n",
    "### <UNK> 인덱스 0으로 설정\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 475, 21, 30, 5297]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 숫자 배정 방식 => 빈도 (3이다? 그 말은 3번째로 많이 등장한 단어)\n",
    "vocab(['<unk>', 'here', 'is', 'an', 'example'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 텍스트 >>> 정수 인코딩\n",
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "\n",
    "### 레이블 >>> 정수 인코딩\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- (3) 데이터 배치(batch)와 반복자 생성\n",
    "    - torch.utils.data.DataLoader : getitem(), len() 구현한 맵 형태(map-style)\n",
    "    - collate_fn() : DataLoader로부터 생성된 샘플 배치 함수\n",
    "        - 입력 : DataLoader에 배치 크기(batch size)가 있는 배치(batch) 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "### DataLoader에서 배치크기만큼 데이터셋 반환하는 함수\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]    # offset : 파일 포인터\n",
    "    \n",
    "    # 뉴스기사, 라벨을 1개씩 추출해서 저장\n",
    "    for (_label, _text) in batch:\n",
    "        # 라벨 인코딩 후 저장\n",
    "        label_list.append(label_pipeline(_label))\n",
    "\n",
    "        # 텍스트 인코딩 후 저장\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "\n",
    "        # 텍스트 offset 즉, 텍스트 크기/길이 저장\n",
    "        offsets.append(processed_text.size(0))\n",
    "    \n",
    "    # 텐서화 진행\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "dataloader = DataLoader(train_iter,\n",
    "                        batch_size=8,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_class : 4     vocab_size : 95811\n"
     ]
    }
   ],
   "source": [
    "### 분류 클래스 수와 단어사전 개수\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f'num_class : {num_class}     vocab_size : {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 1, 3, 3, 1, 3, 3]) tensor([ 3870,     4,   275,  5694,  1653,  3870,   691,     6,  1524,    26,\n",
      "           55,    25,    33,  4185,  5694,  1486,  1653,     4,   378,    43,\n",
      "         2321,    11,    46,  5480,  6492,     1,     2,  3400,  1363,    26,\n",
      "            2,  1969,   648,   393,    34,   299,     6, 18098,     3,   202,\n",
      "         1189, 10312,     2, 17448,     7,     2,   294,    12,     9,    47,\n",
      "          132,     6,   633,     3,   360,  2073,     3,   982,     8, 14922,\n",
      "          187,    20,     5,  6597,  1267,     3,    18,     2,   342,  1395,\n",
      "          177,   945,  3357,    15,   640,   106,   945,   390,    25,  1292,\n",
      "            1,   193,   109,   554,  2946,   193,    12,     9, 42413,   111,\n",
      "           39, 12935, 12004,   566, 18550,     4,   109,     2,   313,   554,\n",
      "         2946,   707,     3,  8019,   123,     6,    40,  5151,   458,     5,\n",
      "         2344,  2014,  5158,   296,     4,   270,     1,   657,   210,   444,\n",
      "          606,    38,   193,   563,    13,    31,    14,    31,    15,    90,\n",
      "          539,    87,     1,    76, 37807, 30839,   703,   110,     2,    23,\n",
      "         7250,  3962,     4,  5094,     2,  1504,    24,   295,    16,     9,\n",
      "          657,  1790,   210,   444,     3,    49,   380,     2,  9097,  6810,\n",
      "           20,     2, 17281,  1151, 11557,   135,     1,     2,   129,     6,\n",
      "         1302,  7950,    30,  9946,     6, 12976,    43,     4,   185,     3,\n",
      "          282,    12,  1531,  4537,  1300,  1041,  1160,  3044,    10,   103,\n",
      "           61,   896,    43,    10,  1300,   649,    24, 11160,     1,   251,\n",
      "          295,    12,     9, 14151,    21, 24571,     7,     2,  1302,  1708,\n",
      "         2002, 11405,     3,    17,    12,     9,   183,   282,    12,  1531,\n",
      "           81,   590,     1,  7253,     3,  7093,     3,  8523,  6764,    29,\n",
      "           89,    13,    31,    14,    31,    15,   461,  1514,  1681,   428,\n",
      "         7253,     3,   527,  8757,  7093,     8,   629,  1038,  1681,  4175,\n",
      "         8523,    79, 11662,    91,   220,     2,  1515,   556,   919,    16,\n",
      "          216,    89,     6,     5,  3446,   234,     1, 27335,   470,  1335,\n",
      "          925,  1008,  1394,    17, 16649,   224,  1008, 29425,     4,    37,\n",
      "         5336,     4,  1235,     3,  1525,     3, 14387,     1,     3,    93,\n",
      "          215,   935,  2920, 14154,   508,     7,     2,  1066,     1,  7354,\n",
      "         1412,  2323,    10, 20752,  1412,  5602,    39,  1987,     5, 24588,\n",
      "         1166, 73920,  7075,    11,  1644,    17,   392,  1202,  2109,   881,\n",
      "           64,     1,    12,     9, 20752,  4359,  2606,     3,  3952,   214,\n",
      "           26,    74,     1]) tensor([  0,  34,  81, 117, 167, 223, 267, 299])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\Text_017_220_38\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:337: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "for labels, texts, offsets in dataloader:\n",
    "    print(labels, texts, offsets)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch_NLP38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
