{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [ 한글 데이터셋 RNN ]\n",
    "- 데이터셋 : Korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Korpora import Korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : e9t@github\n",
      "    Repository : https://github.com/e9t/nsmc\n",
      "    References : www.lucypark.kr/docs/2015-pyconkr/#39\n",
      "\n",
      "    Naver sentiment movie corpus v1.0\n",
      "    This is a movie review dataset in the Korean language.\n",
      "    Reviews were scraped from Naver Movies.\n",
      "\n",
      "    The dataset construction is based on the method noted in\n",
      "    [Large movie review dataset][^1] from Maas et al., 2011.\n",
      "\n",
      "    [^1]: http://ai.stanford.edu/~amaas/data/sentiment/\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\kdp\\Korpora\\nsmc\\ratings_train.txt\n",
      "[Korpora] Corpus `nsmc` is already installed at C:\\Users\\kdp\\Korpora\\nsmc\\ratings_test.txt\n"
     ]
    }
   ],
   "source": [
    "corpus = Korpora.load('nsmc')\n",
    "corpus_df = pd.DataFrame(corpus.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = corpus_df.sample(frac=0.9, random_state=42)     # DF\n",
    "testDF = corpus_df.drop(trainDF.index)                      # DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 45000 entries, 33553 to 6838\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    45000 non-null  object\n",
      " 1   label   45000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "trainDF.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|       | text                                                                                     |   label |\n",
      "|------:|:-----------------------------------------------------------------------------------------|--------:|\n",
      "| 33553 | 모든 편견을 날려 버리는 가슴 따뜻한 영화. 로버트 드 니로, 필립 세이모어 호프만 영원하라. |       1 |\n",
      "|  9427 | 무한 리메이크의 소재. 감독의 역량은 항상 그 자리에...                                    |       0 |\n",
      "|   199 | 신날 것 없는 애니.                                                                       |       0 |\n",
      "| 12447 | 잔잔 격동                                                                                |       1 |\n",
      "| 39489 | 오랜만에 찾은 주말의 명화의 보석                                                         |       1 |\n",
      "Training Data Size : 45000\n",
      "Testing Data Size : 5000\n"
     ]
    }
   ],
   "source": [
    "print(trainDF.head(5).to_markdown())\n",
    "print('Training Data Size :', len(trainDF))\n",
    "print('Testing Data Size :', len(testDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2-1] 토큰화 진행 => 문장 -> 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "### 토큰화 인스턴스 생성\n",
    "tokenizer = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 문장들을 단어 단위로 분리 (stem : 어근 처리)\n",
    "train_tokens = [tokenizer.morphs(text, stem=True) for text in trainDF.text]\n",
    "test_tokens = [tokenizer.morphs(text, stem=True) for text in testDF.text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2-2] 토큰화 => 단어/어휘 사전 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 단어사전 생성 함수\n",
    "def build_vocab(corpus, vocab_size, special_tokens):\n",
    "    counter = Counter()\n",
    "    # 단어/토큰에 대한 빈도수 계산\n",
    "    for tokens in corpus:\n",
    "        counter.update(tokens)\n",
    "    \n",
    "    # 단어/어휘 사전 생성\n",
    "    vocab = special_tokens\n",
    "\n",
    "    # 단어/어휘 사전에 빈도수가 높은 단어 추가\n",
    "    for token, count in counter.most_common(vocab_size):\n",
    "        vocab.append(token)\n",
    "    \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(corpus=train_tokens, vocab_size=5000, special_tokens=[\"<PAD>\", \"<UNK>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VOCAB] -> 5002개\n",
      "\n",
      "['<PAD>', '<UNK>', '.', '이', '영화', '보다', '하다', '의', '..', '에']\n"
     ]
    }
   ],
   "source": [
    "print(f'[VOCAB] -> {len(vocab)}개\\n')\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3] 데이터 가공\n",
    "- 토큰 데이터 정수 인코딩\n",
    "- 데이터 길이 표준화 => 데이터의 길이 맞추기 (1개 문장 구성하는 단어 수 통일)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3-1] 인코딩 & 디코딩 인덱싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 인코딩 : 문자 >>> 숫자로 변환\n",
    "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "\n",
    "### 디코딩 : 숫자 >>> 문자로 변환\n",
    "id_to_token = {idx: token for idx, token in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 단어를 정수로 변환 (단어/어휘 사전에 없는 문자는 <UNK>에 대응하는 숫자[1]로 처리)\n",
    "UNK_ID = token_to_id[\"<UNK>\"]   # 1\n",
    "train_ids = [[token_to_id.get(token, UNK_ID) for token in text] for text in train_tokens]\n",
    "test_ids = [[token_to_id.get(token, UNK_ID) for token in text] for text in test_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3-2] 데이터 구성 단어 수 맞추기 즉, 패딩(padding)\n",
    "- 단어 수 선정 필요\n",
    "- 선저오딘 단어 수에 맞게 데이터 조절 => 길면 잘라내기, 짧으면 채우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 패딩 처리 함수\n",
    "def pad_sequence(sentences, max_length, pad, start='R'):\n",
    "    result = []\n",
    "    for sen in sentences:\n",
    "        sen = sen[:max_length] if start == 'R' else sen[-1 * max_length:]\n",
    "        pad_length = max_length - len(sen)  # sen이 원래 max_length 보다 짧았다면 1이상의 값 저장\n",
    "        padd_sen = sen + [pad]*pad_length if start == 'R' else [pad]*pad_length + sen\n",
    "        result.append(padd_sen)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 학습용, 테스트용 데이터 패딩 처리\n",
    "PAD_ID = token_to_id.get('<PAD>')\n",
    "MAX_LENGTH = 32     # 데이터를 분석해서 결정해야 한다.\n",
    "\n",
    "train_ids = pad_sequence(train_ids, MAX_LENGTH, PAD_ID)     # 일정한 길이의 정수 인코딩 데이터\n",
    "test_ids = pad_sequence(test_ids, MAX_LENGTH, PAD_ID)       # 일정한 길이의 정수 인코딩 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train_ids] -> 32개\n",
      "[test_ids] -> 32개\n"
     ]
    }
   ],
   "source": [
    "print(f'[train_ids] -> {len(train_ids[11])}개')\n",
    "print(f'[test_ids] -> {len(test_ids[11])}개')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96, 4, 788, 305, 75, 650, 96, 125, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[59, 10, 447, 2, 80, 2, 22, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_ids[11])\n",
    "print(test_ids[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4] 학습 준비\n",
    "- Dataset, DataLoader 준비\n",
    "- 학습용/테스트용 함수\n",
    "- 모델 클래스\n",
    "- 학습 관련 변수 => DEVICE, OPTIMIZER, MODEL 인스턴스, EPOCHS, BATCH_SIZE, LOSS_FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'numpy.ndarray'>\n",
      "torch.Size([45000, 32]) torch.Size([45000])\n",
      "torch.Size([5000, 32]) torch.Size([5000])\n"
     ]
    }
   ],
   "source": [
    "### 데이터셋 생성 : List >>> Tensor\n",
    "print(type(train_ids), type(trainDF.label.values))\n",
    "\n",
    "dataTS1 = torch.LongTensor(train_ids)   # 임베딩에 들어가는 것은 Long, Int\n",
    "labelTS1 = torch.FloatTensor(trainDF.label.values)\n",
    "dataTS2 = torch.LongTensor(test_ids)    # 임베딩에 들어가는 것은 Long, Int\n",
    "labelTS2 = torch.FloatTensor(testDF.label.values)\n",
    "\n",
    "print(dataTS1.shape, labelTS1.shape)\n",
    "print(dataTS2.shape, labelTS2.shape)\n",
    "\n",
    "# 데이터셋 생성\n",
    "trainDS = TensorDataset(dataTS1, labelTS1)\n",
    "testDS = TensorDataset(dataTS2, labelTS2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 데이터로더 생성\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "trainDL = DataLoader(trainDS, BATCH_SIZE, shuffle=True)\n",
    "testDL = DataLoader(testDS, BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[4-2] 모델 클래스 정의\n",
    "- 입력층 : Embedding Layer\n",
    "- 은닉층 : RNN/LSTM Layer\n",
    "- 은닉층 : Dropout Layer\n",
    "- 출력층 : Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SentenceClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            vocab_size,\n",
    "            hidden_dim,\n",
    "            embedding_dim,\n",
    "            n_layers,\n",
    "            dropout=0.5,\n",
    "            bidirectional=True,\n",
    "            model_type='lstm'\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=0\n",
    "        )\n",
    "\n",
    "        if model_type == 'rnn':\n",
    "            self.model = nn.RNN(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        elif model_type == 'lstm':\n",
    "            self.model = nn.LSTM(\n",
    "                input_size=embedding_dim,\n",
    "                hidden_size=hidden_dim,\n",
    "                num_layers=n_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "                batch_first=True,\n",
    "            )\n",
    "        \n",
    "        self.classifier = nn.Linear(hidden_dim * 2, 1) if bidirectional else nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        output, _ = self.model(embeddings)\n",
    "        last_output = output[:, -1, :]\n",
    "        last_output = self.dropout(last_output)\n",
    "        logits = self.classifier(last_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEVICE, CLASSIFIER, CRITERION, OPTIMIZER\n",
    "from torch import optim\n",
    "\n",
    "vocab_size = len(token_to_id)\n",
    "hidden_dim = 64\n",
    "embedding_dim = 128\n",
    "n_layers = 2\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "CLASSIFIER = SentenceClassifier(\n",
    "    vocab_size=vocab_size, hidden_dim=hidden_dim, embedding_dim=embedding_dim, n_layers=n_layers\n",
    ")\n",
    "CRITERION = nn.BCEWithLogitsLoss().to(DEVICE)\n",
    "OPTIMIZER = optim.RMSprop(CLASSIFIER.parameters(), lr=0.001)    # Adam() 써도 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, datasets, criterion, optimizer, device, interval):\n",
    "    model.train()       # 학습 모드 ON\n",
    "    losses = list()     # []\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % interval == 0:\n",
    "            print(f'Train Loss {step} : {np.mean(losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, datasets, criterion, device):\n",
    "    model.eval()    # 테스트 모드 ON\n",
    "    losses = list() # []\n",
    "    corrects = list()\n",
    "\n",
    "    for step, (input_ids, labels) in enumerate(datasets):\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        logits = model(input_ids)\n",
    "        loss = criterion(logits, labels)\n",
    "        losses.append(loss.item())\n",
    "        yhat = torch.sigmoid(logits) > 0.5\n",
    "        corrects.extend(torch.eq(yhat, labels).cpu().tolist())\n",
    "    \n",
    "    print(f'Val Loss : {np.mean(losses)}, Val Accuracy : {np.mean(corrects)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH 1]\n",
      "Train Loss 0 : 0.13033577799797058\n",
      "Train Loss 500 : 0.1405233267914065\n",
      "Train Loss 1000 : 0.1470484417350544\n",
      "Val Loss : 0.5373646957669288, Val Accuracy : 0.83\n",
      "[EPOCH 2]\n",
      "Train Loss 0 : 0.1210232824087143\n",
      "Train Loss 500 : 0.12365235728462656\n",
      "Train Loss 1000 : 0.12513974607251324\n",
      "Val Loss : 0.560020245374388, Val Accuracy : 0.8264\n",
      "[EPOCH 3]\n",
      "Train Loss 0 : 0.07615429162979126\n",
      "Train Loss 500 : 0.10692763110964806\n",
      "Train Loss 1000 : 0.110855468873076\n",
      "Val Loss : 0.6034304873578867, Val Accuracy : 0.8318\n",
      "[EPOCH 4]\n",
      "Train Loss 0 : 0.01829984411597252\n",
      "Train Loss 500 : 0.0929033929136342\n",
      "Train Loss 1000 : 0.09915173987948245\n",
      "Val Loss : 0.5860543424726292, Val Accuracy : 0.8272\n",
      "[EPOCH 5]\n",
      "Train Loss 0 : 0.024282090365886688\n",
      "Train Loss 500 : 0.08758405432339289\n",
      "Train Loss 1000 : 0.09086835841284498\n",
      "Val Loss : 0.650977571014386, Val Accuracy : 0.8256\n",
      "[EPOCH 6]\n",
      "Train Loss 0 : 0.0596318244934082\n",
      "Train Loss 500 : 0.08178041104000515\n",
      "Train Loss 1000 : 0.08541374114275976\n",
      "Val Loss : 0.6932387896783793, Val Accuracy : 0.8298\n",
      "[EPOCH 7]\n",
      "Train Loss 0 : 0.06369239091873169\n",
      "Train Loss 500 : 0.07521170350498038\n",
      "Train Loss 1000 : 0.07968112315841161\n",
      "Val Loss : 0.7300153331961602, Val Accuracy : 0.8282\n",
      "[EPOCH 8]\n",
      "Train Loss 0 : 0.15252302587032318\n",
      "Train Loss 500 : 0.06712424928534352\n",
      "Train Loss 1000 : 0.07097434308044948\n",
      "Val Loss : 0.6954269666153534, Val Accuracy : 0.8238\n",
      "[EPOCH 9]\n",
      "Train Loss 0 : 0.04155693203210831\n",
      "Train Loss 500 : 0.06299399568823849\n",
      "Train Loss 1000 : 0.06579135301254027\n",
      "Val Loss : 0.7051316088029913, Val Accuracy : 0.8256\n",
      "[EPOCH 10]\n",
      "Train Loss 0 : 0.08618238568305969\n",
      "Train Loss 500 : 0.05852351806818056\n",
      "Train Loss 1000 : 0.06244973660763967\n",
      "Val Loss : 0.7864485329883114, Val Accuracy : 0.8216\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "interval = 500\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'[EPOCH {epoch+1}]')\n",
    "    train(CLASSIFIER, trainDL, CRITERION, OPTIMIZER, DEVICE, interval)\n",
    "    test(CLASSIFIER, testDL, CRITERION, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Text_017_220_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
